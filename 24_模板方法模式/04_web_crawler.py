"""
04_web_crawler.py - ç½‘ç»œçˆ¬è™«ç³»ç»Ÿçš„æ¨¡æ¿æ–¹æ³•è®¾è®¡

è¿™ä¸ªç¤ºä¾‹å±•ç¤ºäº†åœ¨ç½‘ç»œçˆ¬è™«ç³»ç»Ÿä¸­å¦‚ä½•ä½¿ç”¨æ¨¡æ¿æ–¹æ³•æ¨¡å¼ï¼š
- ç»Ÿä¸€çš„çˆ¬è™«å¤„ç†æµç¨‹
- ä¸åŒç½‘ç«™çš„å…·ä½“çˆ¬å–ç­–ç•¥
- çµæ´»çš„æ•°æ®å¤„ç†å’Œå­˜å‚¨æœºåˆ¶
- é”™è¯¯å¤„ç†å’Œé‡è¯•æœºåˆ¶
"""

from abc import ABC, abstractmethod
import time
import random
from typing import List, Dict, Any, Optional
from urllib.parse import urljoin, urlparse
import json


# ==================== æŠ½è±¡ç½‘ç»œçˆ¬è™« ====================
class WebCrawler(ABC):
    """ç½‘ç»œçˆ¬è™«æ¨¡æ¿ç±»
    
    å®šä¹‰äº†ç½‘ç»œçˆ¬è™«çš„æ ‡å‡†æµç¨‹ï¼š
    1. åˆå§‹åŒ–çˆ¬è™«é…ç½®
    2. è·å–ç›®æ ‡URLåˆ—è¡¨
    3. çˆ¬å–ç½‘é¡µå†…å®¹
    4. è§£æå’Œæå–æ•°æ®
    5. æ•°æ®æ¸…æ´—å’ŒéªŒè¯
    6. ä¿å­˜æ•°æ®
    7. æ¸…ç†èµ„æº
    """
    
    def __init__(self, name: str, base_url: str):
        self.name = name
        self.base_url = base_url
        self.crawled_urls = set()
        self.failed_urls = set()
        self.extracted_data = []
        self.start_time = None
        self.request_count = 0
        self.success_count = 0
    
    def crawl(self, max_pages: int = 10) -> Dict[str, Any]:
        """æ¨¡æ¿æ–¹æ³• - å®šä¹‰å®Œæ•´çš„çˆ¬è™«æµç¨‹"""
        print(f"ğŸ•·ï¸  å¼€å§‹çˆ¬è™«ä»»åŠ¡: {self.name}")
        print(f"ç›®æ ‡ç½‘ç«™: {self.base_url}")
        print("-" * 60)
        
        self.start_time = time.time()
        
        try:
            # 1. åˆå§‹åŒ–çˆ¬è™«é…ç½®
            print("âš™ï¸  æ­¥éª¤1: åˆå§‹åŒ–çˆ¬è™«é…ç½®")
            self.initialize_crawler()
            print("âœ… çˆ¬è™«é…ç½®å®Œæˆ")
            
            # 2. è·å–ç›®æ ‡URLåˆ—è¡¨
            print("\nğŸ”— æ­¥éª¤2: è·å–ç›®æ ‡URLåˆ—è¡¨")
            target_urls = self.get_target_urls(max_pages)
            print(f"âœ… è·å¾— {len(target_urls)} ä¸ªç›®æ ‡URL")
            
            # 3. çˆ¬å–ç½‘é¡µå†…å®¹
            print("\nğŸ“¥ æ­¥éª¤3: çˆ¬å–ç½‘é¡µå†…å®¹")
            for i, url in enumerate(target_urls, 1):
                print(f"æ­£åœ¨çˆ¬å– ({i}/{len(target_urls)}): {url}")
                
                if self.should_crawl_url(url):
                    page_content = self.fetch_page_content(url)
                    if page_content:
                        # 4. è§£æå’Œæå–æ•°æ®
                        extracted_items = self.extract_data(url, page_content)
                        if extracted_items:
                            # 5. æ•°æ®æ¸…æ´—å’ŒéªŒè¯
                            cleaned_items = self.clean_and_validate_data(extracted_items)
                            self.extracted_data.extend(cleaned_items)
                            self.success_count += 1
                            print(f"âœ… æˆåŠŸæå– {len(cleaned_items)} æ¡æ•°æ®")
                        else:
                            print("âš ï¸  æœªæå–åˆ°æœ‰æ•ˆæ•°æ®")
                    else:
                        self.failed_urls.add(url)
                        print("âŒ é¡µé¢è·å–å¤±è´¥")
                else:
                    print("â­ï¸  è·³è¿‡æ­¤URL")
                
                self.crawled_urls.add(url)
                
                # å»¶è¿Ÿè¯·æ±‚ï¼Œé¿å…è¿‡äºé¢‘ç¹
                if self.should_delay():
                    delay = self.get_request_delay()
                    print(f"â±ï¸  å»¶è¿Ÿ {delay} ç§’...")
                    time.sleep(delay)
            
            # 6. ä¿å­˜æ•°æ®
            print(f"\nğŸ’¾ æ­¥éª¤4: ä¿å­˜æ•°æ®")
            if self.extracted_data:
                self.save_data(self.extracted_data)
                print(f"âœ… ä¿å­˜äº† {len(self.extracted_data)} æ¡æ•°æ®")
            else:
                print("âš ï¸  æ²¡æœ‰æ•°æ®éœ€è¦ä¿å­˜")
            
            # 7. æ¸…ç†èµ„æº
            print("\nğŸ§¹ æ­¥éª¤5: æ¸…ç†èµ„æº")
            self.cleanup_crawler()
            
            # ç”Ÿæˆçˆ¬è™«æŠ¥å‘Š
            report = self.generate_crawl_report()
            
            print("-" * 60)
            print("ğŸ‰ çˆ¬è™«ä»»åŠ¡å®Œæˆ!")
            return report
            
        except Exception as e:
            print(f"âŒ çˆ¬è™«ä»»åŠ¡å¤±è´¥: {str(e)}")
            return {"status": "failed", "error": str(e)}
    
    # æŠ½è±¡æ–¹æ³• - å­ç±»å¿…é¡»å®ç°
    @abstractmethod
    def get_target_urls(self, max_pages: int) -> List[str]:
        """è·å–ç›®æ ‡URLåˆ—è¡¨"""
        pass
    
    @abstractmethod
    def extract_data(self, url: str, content: str) -> List[Dict[str, Any]]:
        """ä»é¡µé¢å†…å®¹ä¸­æå–æ•°æ®"""
        pass
    
    # å…·ä½“æ–¹æ³• - æä¾›é»˜è®¤å®ç°
    def initialize_crawler(self):
        """åˆå§‹åŒ–çˆ¬è™«é…ç½® - é»˜è®¤å®ç°"""
        print("è®¾ç½®è¯·æ±‚å¤´...")
        print("é…ç½®ä»£ç†è®¾ç½®...")
        print("åˆå§‹åŒ–ä¼šè¯...")
    
    def fetch_page_content(self, url: str) -> Optional[str]:
        """è·å–é¡µé¢å†…å®¹ - æ¨¡æ‹Ÿå®ç°"""
        self.request_count += 1
        
        # æ¨¡æ‹Ÿç½‘ç»œè¯·æ±‚
        print(f"å‘é€HTTPè¯·æ±‚åˆ°: {url}")
        
        # æ¨¡æ‹Ÿè¯·æ±‚å¯èƒ½å¤±è´¥
        if random.random() < 0.1:  # 10%å¤±è´¥ç‡
            print("âŒ ç½‘ç»œè¯·æ±‚å¤±è´¥")
            return None
        
        # æ¨¡æ‹Ÿé¡µé¢å†…å®¹
        mock_content = f"""
        <html>
            <head><title>é¡µé¢æ ‡é¢˜ - {url}</title></head>
            <body>
                <h1>è¿™æ˜¯æ¥è‡ª {url} çš„å†…å®¹</h1>
                <div class="content">
                    <p>è¿™æ˜¯ä¸€äº›ç¤ºä¾‹å†…å®¹...</p>
                    <span class="price">Â¥{random.randint(10, 1000)}</span>
                    <div class="description">äº§å“æè¿°ä¿¡æ¯</div>
                </div>
            </body>
        </html>
        """
        
        print("âœ… é¡µé¢å†…å®¹è·å–æˆåŠŸ")
        return mock_content
    
    def clean_and_validate_data(self, data: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """æ•°æ®æ¸…æ´—å’ŒéªŒè¯ - é»˜è®¤å®ç°"""
        cleaned_data = []
        for item in data:
            # ç§»é™¤ç©ºå€¼å’Œæ— æ•ˆæ•°æ®
            cleaned_item = {k: v for k, v in item.items() if v and str(v).strip()}
            if cleaned_item:
                cleaned_data.append(cleaned_item)
        
        print(f"æ•°æ®æ¸…æ´—: {len(data)} -> {len(cleaned_data)}")
        return cleaned_data
    
    def save_data(self, data: List[Dict[str, Any]]):
        """ä¿å­˜æ•°æ® - é»˜è®¤å®ç°"""
        filename = f"{self.name.replace(' ', '_')}_data.json"
        print(f"ä¿å­˜æ•°æ®åˆ°æ–‡ä»¶: {filename}")
        
        # æ¨¡æ‹Ÿä¿å­˜è¿‡ç¨‹
        print(f"å†™å…¥ {len(data)} æ¡è®°å½•")
        if data:
            print(f"æ ·ä¾‹æ•°æ®: {json.dumps(data[0], ensure_ascii=False, indent=2)}")
    
    def cleanup_crawler(self):
        """æ¸…ç†çˆ¬è™«èµ„æº - é»˜è®¤å®ç°"""
        print("å…³é—­ç½‘ç»œè¿æ¥...")
        print("æ¸…ç†ä¸´æ—¶æ–‡ä»¶...")
    
    def generate_crawl_report(self) -> Dict[str, Any]:
        """ç”Ÿæˆçˆ¬è™«æŠ¥å‘Š"""
        end_time = time.time()
        duration = round(end_time - self.start_time, 2)
        
        report = {
            "crawler_name": self.name,
            "base_url": self.base_url,
            "total_requests": self.request_count,
            "successful_pages": self.success_count,
            "failed_pages": len(self.failed_urls),
            "extracted_items": len(self.extracted_data),
            "duration_seconds": duration,
            "success_rate": round(self.success_count / max(self.request_count, 1) * 100, 1),
            "timestamp": time.strftime('%Y-%m-%d %H:%M:%S')
        }
        
        print(f"çˆ¬è™«åç§°: {report['crawler_name']}")
        print(f"æ€»è¯·æ±‚æ•°: {report['total_requests']}")
        print(f"æˆåŠŸé¡µé¢: {report['successful_pages']}")
        print(f"å¤±è´¥é¡µé¢: {report['failed_pages']}")
        print(f"æå–æ•°æ®: {report['extracted_items']} æ¡")
        print(f"æˆåŠŸç‡: {report['success_rate']}%")
        print(f"æ€»è€—æ—¶: {report['duration_seconds']} ç§’")
        
        return report
    
    # é’©å­æ–¹æ³• - å­ç±»å¯é€‰æ‹©é‡å†™
    def should_crawl_url(self, url: str) -> bool:
        """æ˜¯å¦åº”è¯¥çˆ¬å–æ­¤URL"""
        return url not in self.crawled_urls
    
    def should_delay(self) -> bool:
        """æ˜¯å¦éœ€è¦å»¶è¿Ÿè¯·æ±‚"""
        return True
    
    def get_request_delay(self) -> float:
        """è·å–è¯·æ±‚å»¶è¿Ÿæ—¶é—´"""
        return random.uniform(0.5, 2.0)


# ==================== å…·ä½“çˆ¬è™«å®ç° ====================
class EcommerceCrawler(WebCrawler):
    """ç”µå•†ç½‘ç«™çˆ¬è™«"""
    
    def __init__(self):
        super().__init__("ç”µå•†äº§å“çˆ¬è™«", "https://example-shop.com")
    
    def get_target_urls(self, max_pages: int) -> List[str]:
        """è·å–ç”µå•†äº§å“é¡µé¢URL"""
        print("ç”Ÿæˆäº§å“é¡µé¢URL...")
        
        # æ¨¡æ‹Ÿç”Ÿæˆäº§å“é¡µé¢URL
        urls = []
        for i in range(1, min(max_pages + 1, 11)):
            urls.append(f"{self.base_url}/products/page/{i}")
            urls.append(f"{self.base_url}/category/electronics/page/{i}")
        
        return urls[:max_pages]
    
    def extract_data(self, url: str, content: str) -> List[Dict[str, Any]]:
        """æå–äº§å“ä¿¡æ¯"""
        print("è§£æäº§å“ä¿¡æ¯...")
        
        # æ¨¡æ‹Ÿä»HTMLä¸­æå–äº§å“æ•°æ®
        products = []
        for i in range(random.randint(1, 5)):
            product = {
                "name": f"äº§å“ {random.randint(1000, 9999)}",
                "price": random.randint(50, 2000),
                "category": random.choice(["ç”µå­äº§å“", "æœè£…", "å®¶å±…", "å›¾ä¹¦"]),
                "rating": round(random.uniform(3.0, 5.0), 1),
                "stock": random.randint(0, 100),
                "url": url,
                "crawl_time": time.strftime('%Y-%m-%d %H:%M:%S')
            }
            products.append(product)
        
        return products
    
    def clean_and_validate_data(self, data: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """ç”µå•†æ•°æ®ç‰¹æ®Šæ¸…æ´—"""
        cleaned_data = super().clean_and_validate_data(data)
        
        # è¿‡æ»¤æ‰æ— åº“å­˜æˆ–ä»·æ ¼å¼‚å¸¸çš„äº§å“
        valid_products = []
        for product in cleaned_data:
            if (product.get('stock', 0) > 0 and 
                product.get('price', 0) > 0 and
                product.get('rating', 0) >= 3.0):
                valid_products.append(product)
        
        print(f"ç”µå•†æ•°æ®éªŒè¯: {len(cleaned_data)} -> {len(valid_products)}")
        return valid_products
    
    def get_request_delay(self) -> float:
        """ç”µå•†ç½‘ç«™éœ€è¦æ›´é•¿å»¶è¿Ÿ"""
        return random.uniform(1.0, 3.0)


class NewsCrawler(WebCrawler):
    """æ–°é—»ç½‘ç«™çˆ¬è™«"""
    
    def __init__(self):
        super().__init__("æ–°é—»æ–‡ç« çˆ¬è™«", "https://example-news.com")
    
    def get_target_urls(self, max_pages: int) -> List[str]:
        """è·å–æ–°é—»æ–‡ç« URL"""
        print("ç”Ÿæˆæ–°é—»æ–‡ç« URL...")
        
        # æ¨¡æ‹Ÿç”Ÿæˆæ–°é—»é¡µé¢URL
        urls = []
        categories = ["tech", "business", "sports", "entertainment"]
        
        for category in categories:
            for i in range(1, max_pages // len(categories) + 2):
                urls.append(f"{self.base_url}/{category}/page/{i}")
        
        return urls[:max_pages]
    
    def extract_data(self, url: str, content: str) -> List[Dict[str, Any]]:
        """æå–æ–°é—»ä¿¡æ¯"""
        print("è§£ææ–°é—»æ–‡ç« ...")
        
        # æ¨¡æ‹Ÿä»HTMLä¸­æå–æ–°é—»æ•°æ®
        articles = []
        for i in range(random.randint(2, 8)):
            article = {
                "title": f"æ–°é—»æ ‡é¢˜ {random.randint(1000, 9999)}",
                "author": random.choice(["å¼ è®°è€…", "æç¼–è¾‘", "ç‹é€šè®¯å‘˜", "èµµä¸»ç¼–"]),
                "publish_date": time.strftime('%Y-%m-%d'),
                "category": random.choice(["ç§‘æŠ€", "è´¢ç»", "ä½“è‚²", "å¨±ä¹"]),
                "summary": f"è¿™æ˜¯ä¸€ç¯‡å…³äº...çš„æ–°é—»æ‘˜è¦ {random.randint(100, 999)}",
                "word_count": random.randint(500, 3000),
                "url": url,
                "crawl_time": time.strftime('%Y-%m-%d %H:%M:%S')
            }
            articles.append(article)
        
        return articles
    
    def should_delay(self) -> bool:
        """æ–°é—»ç½‘ç«™å¯ä»¥æ›´é¢‘ç¹åœ°è¯·æ±‚"""
        return random.random() < 0.7  # 70%æ¦‚ç‡å»¶è¿Ÿ
    
    def get_request_delay(self) -> float:
        """æ–°é—»ç½‘ç«™å»¶è¿Ÿè¾ƒçŸ­"""
        return random.uniform(0.3, 1.5)


class SocialMediaCrawler(WebCrawler):
    """ç¤¾äº¤åª’ä½“çˆ¬è™«"""
    
    def __init__(self):
        super().__init__("ç¤¾äº¤åª’ä½“çˆ¬è™«", "https://example-social.com")
        self.api_rate_limit = 0
    
    def initialize_crawler(self):
        """ç¤¾äº¤åª’ä½“çˆ¬è™«ç‰¹æ®Šåˆå§‹åŒ–"""
        super().initialize_crawler()
        print("é…ç½®APIå¯†é’¥...")
        print("è®¾ç½®OAuthè®¤è¯...")
        self.api_rate_limit = 100  # æ¨¡æ‹ŸAPIé™åˆ¶
    
    def get_target_urls(self, max_pages: int) -> List[str]:
        """è·å–ç¤¾äº¤åª’ä½“APIç«¯ç‚¹"""
        print("ç”ŸæˆAPIè¯·æ±‚URL...")
        
        # æ¨¡æ‹ŸAPIç«¯ç‚¹
        urls = []
        for i in range(max_pages):
            urls.append(f"{self.base_url}/api/posts?page={i+1}")
        
        return urls
    
    def extract_data(self, url: str, content: str) -> List[Dict[str, Any]]:
        """æå–ç¤¾äº¤åª’ä½“æ•°æ®"""
        print("è§£æç¤¾äº¤åª’ä½“æ•°æ®...")
        
        # æ¨¡æ‹ŸAPIå“åº”æ•°æ®
        posts = []
        for i in range(random.randint(10, 20)):
            post = {
                "post_id": f"post_{random.randint(100000, 999999)}",
                "user": f"user_{random.randint(1000, 9999)}",
                "content": f"è¿™æ˜¯ä¸€æ¡ç¤¾äº¤åª’ä½“å†…å®¹ #{random.randint(1, 100)}",
                "likes": random.randint(0, 1000),
                "shares": random.randint(0, 100),
                "comments": random.randint(0, 50),
                "timestamp": time.strftime('%Y-%m-%d %H:%M:%S'),
                "hashtags": [f"#æ ‡ç­¾{i}" for i in range(random.randint(1, 4))],
                "url": url
            }
            posts.append(post)
        
        return posts
    
    def should_crawl_url(self, url: str) -> bool:
        """æ£€æŸ¥APIé™åˆ¶"""
        if self.api_rate_limit <= 0:
            print("âš ï¸  APIé™åˆ¶å·²è¾¾ä¸Šé™ï¼Œè·³è¿‡è¯·æ±‚")
            return False
        
        self.api_rate_limit -= 1
        return super().should_crawl_url(url)
    
    def clean_and_validate_data(self, data: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """ç¤¾äº¤åª’ä½“æ•°æ®ç‰¹æ®Šæ¸…æ´—"""
        cleaned_data = super().clean_and_validate_data(data)
        
        # è¿‡æ»¤æ‰äº’åŠ¨é‡è¿‡ä½çš„å¸–å­
        popular_posts = []
        for post in cleaned_data:
            total_engagement = post.get('likes', 0) + post.get('shares', 0) + post.get('comments', 0)
            if total_engagement >= 5:  # è‡³å°‘5ä¸ªäº’åŠ¨
                popular_posts.append(post)
        
        print(f"ç¤¾äº¤åª’ä½“æ•°æ®è¿‡æ»¤: {len(cleaned_data)} -> {len(popular_posts)}")
        return popular_posts
    
    def get_request_delay(self) -> float:
        """APIè¯·æ±‚éœ€è¦éµå®ˆé™åˆ¶"""
        return random.uniform(2.0, 4.0)


# ==================== æ¼”ç¤ºå‡½æ•° ====================
def demo_web_crawlers():
    """ç½‘ç»œçˆ¬è™«æ¼”ç¤º"""
    print("=" * 80)
    print("ğŸ•·ï¸  ç½‘ç»œçˆ¬è™«ç³»ç»Ÿæ¨¡æ¿æ–¹æ³•æ¼”ç¤º")
    print("=" * 80)
    
    # åˆ›å»ºä¸åŒç±»å‹çš„çˆ¬è™«
    crawlers = [
        EcommerceCrawler(),
        NewsCrawler(),
        SocialMediaCrawler()
    ]
    
    max_pages_per_crawler = 5
    all_reports = []
    
    # æ‰§è¡Œçˆ¬è™«ä»»åŠ¡
    for crawler in crawlers:
        print(f"\n{'='*20} {crawler.name} {'='*20}")
        report = crawler.crawl(max_pages_per_crawler)
        all_reports.append(report)
        time.sleep(1)
    
    # æ±‡æ€»æŠ¥å‘Š
    print("\n" + "="*80)
    print("ğŸ“Š çˆ¬è™«ä»»åŠ¡æ±‡æ€»æŠ¥å‘Š")
    print("="*80)
    
    total_requests = sum(r.get('total_requests', 0) for r in all_reports)
    total_items = sum(r.get('extracted_items', 0) for r in all_reports)
    total_duration = sum(r.get('duration_seconds', 0) for r in all_reports)
    avg_success_rate = sum(r.get('success_rate', 0) for r in all_reports) / len(all_reports)
    
    print(f"æ€»çˆ¬è™«æ•°é‡: {len(crawlers)}")
    print(f"æ€»è¯·æ±‚æ•°é‡: {total_requests}")
    print(f"æ€»æå–æ•°æ®: {total_items} æ¡")
    print(f"æ€»è€—æ—¶: {round(total_duration, 2)} ç§’")
    print(f"å¹³å‡æˆåŠŸç‡: {round(avg_success_rate, 1)}%")
    
    print("\nğŸ“‹ å„çˆ¬è™«è¯¦æƒ…:")
    for report in all_reports:
        print(f"  {report['crawler_name']}: {report['extracted_items']}æ¡æ•°æ®, "
              f"{report['success_rate']}%æˆåŠŸç‡")


if __name__ == "__main__":
    demo_web_crawlers()
