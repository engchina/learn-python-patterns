#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
è¿­ä»£å™¨æ¨¡å¼çš„å®é™…åº”ç”¨åœºæ™¯

æœ¬æ¨¡å—æ¼”ç¤ºäº†è¿­ä»£å™¨åœ¨å®é™…é¡¹ç›®ä¸­çš„åº”ç”¨ï¼ŒåŒ…æ‹¬ï¼š
1. ç½‘ç»œçˆ¬è™«æ•°æ®è¿­ä»£ - åˆ†é¡µAPIæ•°æ®è·å–
2. æ—¥å¿—æ–‡ä»¶åˆ†æ - å¤§æ–‡ä»¶é€è¡Œå¤„ç†
3. æ•°æ®åº“ç»“æœé›†è¿­ä»£ - æ‰¹é‡æ•°æ®å¤„ç†
4. é…ç½®æ–‡ä»¶è§£æ - ç»“æ„åŒ–æ•°æ®è¯»å–

ä½œè€…: Assistant
æ—¥æœŸ: 2024-01-20
"""

import json
import re
import time
import random
from datetime import datetime, timedelta
from typing import Dict, List, Any, Optional, Iterator, Generator
from urllib.parse import urljoin
import requests
from dataclasses import dataclass


@dataclass
class LogEntry:
    """æ—¥å¿—æ¡ç›®æ•°æ®ç±»"""
    timestamp: datetime
    level: str
    message: str
    source: str
    user_id: Optional[str] = None
    ip_address: Optional[str] = None
    
    def to_dict(self) -> Dict[str, Any]:
        return {
            'timestamp': self.timestamp.isoformat(),
            'level': self.level,
            'message': self.message,
            'source': self.source,
            'user_id': self.user_id,
            'ip_address': self.ip_address
        }


class WebCrawlerIterator:
    """ç½‘ç»œçˆ¬è™«è¿­ä»£å™¨ - æ¨¡æ‹Ÿåˆ†é¡µAPIæ•°æ®è·å–"""
    
    def __init__(self, base_url: str, total_pages: int = 5, page_size: int = 10):
        self.base_url = base_url
        self.total_pages = total_pages
        self.page_size = page_size
        self.current_page = 0
        self.items_fetched = 0
        self.session = self._create_session()
    
    def _create_session(self):
        """åˆ›å»ºHTTPä¼šè¯ï¼ˆæ¨¡æ‹Ÿï¼‰"""
        # åœ¨å®é™…åº”ç”¨ä¸­ï¼Œè¿™é‡Œä¼šåˆ›å»ºçœŸå®çš„requests.Session
        return {"connected": True, "retry_count": 0}
    
    def _fetch_page(self, page: int) -> List[Dict[str, Any]]:
        """è·å–æŒ‡å®šé¡µé¢çš„æ•°æ®ï¼ˆæ¨¡æ‹Ÿï¼‰"""
        # æ¨¡æ‹Ÿç½‘ç»œå»¶è¿Ÿ
        time.sleep(0.1)
        
        # æ¨¡æ‹ŸAPIå“åº”
        items = []
        start_id = page * self.page_size + 1
        
        for i in range(self.page_size):
            item_id = start_id + i
            items.append({
                'id': item_id,
                'title': f'æ–‡ç« æ ‡é¢˜_{item_id}',
                'content': f'è¿™æ˜¯ç¬¬{item_id}ç¯‡æ–‡ç« çš„å†…å®¹...',
                'author': f'ä½œè€…_{item_id % 5 + 1}',
                'created_at': (datetime.now() - timedelta(days=item_id)).isoformat(),
                'tags': [f'æ ‡ç­¾{item_id % 3 + 1}', f'æ ‡ç­¾{item_id % 4 + 1}'],
                'view_count': random.randint(100, 10000)
            })
        
        print(f"ğŸŒ è·å–ç¬¬ {page + 1} é¡µæ•°æ®ï¼Œ{len(items)} æ¡è®°å½•")
        return items
    
    def __iter__(self) -> 'WebCrawlerIterator':
        """é‡ç½®è¿­ä»£å™¨çŠ¶æ€"""
        self.current_page = 0
        self.items_fetched = 0
        return self
    
    def __next__(self) -> Dict[str, Any]:
        """è¿”å›ä¸‹ä¸€ä¸ªæ•°æ®é¡¹"""
        if self.current_page >= self.total_pages:
            print(f"ğŸ•·ï¸ çˆ¬è™«å®Œæˆï¼Œå…±è·å– {self.items_fetched} æ¡æ•°æ®")
            raise StopIteration
        
        # å¦‚æœå½“å‰é¡µé¢æ²¡æœ‰æ•°æ®ï¼Œè·å–ä¸‹ä¸€é¡µ
        if not hasattr(self, '_current_page_data') or not self._current_page_data:
            try:
                self._current_page_data = self._fetch_page(self.current_page)
                self.current_page += 1
            except Exception as e:
                print(f"âŒ è·å–æ•°æ®å¤±è´¥: {e}")
                raise StopIteration
        
        # è¿”å›å½“å‰é¡µé¢çš„ä¸‹ä¸€ä¸ªé¡¹ç›®
        if self._current_page_data:
            item = self._current_page_data.pop(0)
            self.items_fetched += 1
            return item
        else:
            raise StopIteration


class LogFileIterator:
    """æ—¥å¿—æ–‡ä»¶è¿­ä»£å™¨ - é€è¡Œè§£ææ—¥å¿—æ–‡ä»¶"""
    
    def __init__(self, log_pattern: str = None):
        self.log_pattern = log_pattern or self._default_log_pattern()
        self.parsed_count = 0
        self.error_count = 0
    
    def _default_log_pattern(self) -> str:
        """é»˜è®¤æ—¥å¿—æ ¼å¼æ­£åˆ™è¡¨è¾¾å¼"""
        # åŒ¹é…æ ¼å¼: 2024-01-20 10:30:45 [INFO] source: message (user:123, ip:192.168.1.1)
        return (
            r'(?P<timestamp>\d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2})\s+'
            r'\[(?P<level>\w+)\]\s+'
            r'(?P<source>\w+):\s+'
            r'(?P<message>.*?)'
            r'(?:\s+\(user:(?P<user_id>\w+),\s+ip:(?P<ip_address>[\d.]+)\))?'
        )
    
    def parse_log_line(self, line: str) -> Optional[LogEntry]:
        """è§£æå•è¡Œæ—¥å¿—"""
        line = line.strip()
        if not line or line.startswith('#'):
            return None
        
        match = re.match(self.log_pattern, line)
        if match:
            try:
                timestamp = datetime.strptime(match.group('timestamp'), '%Y-%m-%d %H:%M:%S')
                return LogEntry(
                    timestamp=timestamp,
                    level=match.group('level'),
                    message=match.group('message'),
                    source=match.group('source'),
                    user_id=match.group('user_id'),
                    ip_address=match.group('ip_address')
                )
            except ValueError as e:
                self.error_count += 1
                print(f"âš ï¸ è§£ææ—¶é—´æˆ³å¤±è´¥: {e}")
                return None
        else:
            self.error_count += 1
            return None
    
    def process_file(self, filename: str) -> Generator[LogEntry, None, None]:
        """å¤„ç†æ—¥å¿—æ–‡ä»¶"""
        try:
            with open(filename, 'r', encoding='utf-8') as file:
                for line_num, line in enumerate(file, 1):
                    log_entry = self.parse_log_line(line)
                    if log_entry:
                        self.parsed_count += 1
                        yield log_entry
                    
                    # æ¯å¤„ç†1000è¡Œæ˜¾ç¤ºè¿›åº¦
                    if line_num % 1000 == 0:
                        print(f"ğŸ“Š å·²å¤„ç† {line_num} è¡Œï¼Œè§£ææˆåŠŸ {self.parsed_count} æ¡")
        
        except FileNotFoundError:
            print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {filename}")
        except Exception as e:
            print(f"âŒ å¤„ç†æ–‡ä»¶å¤±è´¥: {e}")
        finally:
            print(f"ğŸ“‹ å¤„ç†å®Œæˆ: æˆåŠŸ {self.parsed_count} æ¡ï¼Œé”™è¯¯ {self.error_count} æ¡")


class DatabaseBatchIterator:
    """æ•°æ®åº“æ‰¹é‡è¿­ä»£å™¨ - æ¨¡æ‹Ÿå¤§æ•°æ®é›†çš„æ‰¹é‡å¤„ç†"""
    
    def __init__(self, table_name: str, batch_size: int = 1000, where_clause: str = ""):
        self.table_name = table_name
        self.batch_size = batch_size
        self.where_clause = where_clause
        self.offset = 0
        self.total_processed = 0
        self.connection = self._mock_connection()
    
    def _mock_connection(self) -> Dict[str, Any]:
        """æ¨¡æ‹Ÿæ•°æ®åº“è¿æ¥"""
        return {
            'connected': True,
            'total_records': 10000,  # æ¨¡æ‹Ÿæ€»è®°å½•æ•°
            'query_count': 0
        }
    
    def _execute_query(self, offset: int, limit: int) -> List[Dict[str, Any]]:
        """æ‰§è¡ŒæŸ¥è¯¢ï¼ˆæ¨¡æ‹Ÿï¼‰"""
        if not self.connection['connected']:
            raise Exception("æ•°æ®åº“è¿æ¥å·²æ–­å¼€")
        
        # æ¨¡æ‹ŸæŸ¥è¯¢å»¶è¿Ÿ
        time.sleep(0.05)
        
        self.connection['query_count'] += 1
        
        # è®¡ç®—å®é™…è¿”å›çš„è®°å½•æ•°
        remaining = self.connection['total_records'] - offset
        actual_limit = min(limit, remaining)
        
        if actual_limit <= 0:
            return []
        
        # ç”Ÿæˆæ¨¡æ‹Ÿæ•°æ®
        records = []
        for i in range(actual_limit):
            record_id = offset + i + 1
            records.append({
                'id': record_id,
                'name': f'ç”¨æˆ·_{record_id}',
                'email': f'user{record_id}@example.com',
                'age': 20 + (record_id % 50),
                'department': f'éƒ¨é—¨_{record_id % 10 + 1}',
                'salary': 50000 + (record_id % 100) * 1000,
                'created_at': (datetime.now() - timedelta(days=record_id)).isoformat()
            })
        
        print(f"ğŸ—„ï¸ æ‰§è¡ŒæŸ¥è¯¢ #{self.connection['query_count']}: åç§» {offset}, è·å– {len(records)} æ¡è®°å½•")
        return records
    
    def __iter__(self) -> 'DatabaseBatchIterator':
        """é‡ç½®è¿­ä»£å™¨çŠ¶æ€"""
        self.offset = 0
        self.total_processed = 0
        return self
    
    def __next__(self) -> List[Dict[str, Any]]:
        """è¿”å›ä¸‹ä¸€æ‰¹æ•°æ®"""
        batch = self._execute_query(self.offset, self.batch_size)
        
        if not batch:
            print(f"ğŸ“Š æ•°æ®åº“è¿­ä»£å®Œæˆï¼Œå…±å¤„ç† {self.total_processed} æ¡è®°å½•")
            raise StopIteration
        
        self.offset += len(batch)
        self.total_processed += len(batch)
        
        return batch
    
    def close(self) -> None:
        """å…³é—­æ•°æ®åº“è¿æ¥"""
        self.connection['connected'] = False
        print(f"ğŸ”Œ æ•°æ®åº“è¿æ¥å·²å…³é—­ï¼Œå…±æ‰§è¡Œ {self.connection['query_count']} æ¬¡æŸ¥è¯¢")


class ConfigFileIterator:
    """é…ç½®æ–‡ä»¶è¿­ä»£å™¨ - è§£æç»“æ„åŒ–é…ç½®"""
    
    def __init__(self, config_format: str = 'json'):
        self.config_format = config_format.lower()
        self.sections_processed = 0
    
    def parse_json_config(self, content: str) -> Generator[Dict[str, Any], None, None]:
        """è§£æJSONé…ç½®"""
        try:
            config = json.loads(content)
            for section_name, section_data in config.items():
                self.sections_processed += 1
                yield {
                    'section': section_name,
                    'type': 'json',
                    'data': section_data,
                    'keys': list(section_data.keys()) if isinstance(section_data, dict) else []
                }
        except json.JSONDecodeError as e:
            print(f"âŒ JSONè§£æå¤±è´¥: {e}")
    
    def parse_ini_config(self, content: str) -> Generator[Dict[str, Any], None, None]:
        """è§£æINIé…ç½®"""
        current_section = None
        section_data = {}
        
        for line_num, line in enumerate(content.split('\n'), 1):
            line = line.strip()
            
            # è·³è¿‡ç©ºè¡Œå’Œæ³¨é‡Š
            if not line or line.startswith('#') or line.startswith(';'):
                continue
            
            # æ£€æŸ¥æ˜¯å¦æ˜¯èŠ‚æ ‡é¢˜
            if line.startswith('[') and line.endswith(']'):
                # å¦‚æœæœ‰ä¹‹å‰çš„èŠ‚ï¼Œå…ˆè¿”å›
                if current_section:
                    self.sections_processed += 1
                    yield {
                        'section': current_section,
                        'type': 'ini',
                        'data': section_data.copy(),
                        'keys': list(section_data.keys())
                    }
                
                # å¼€å§‹æ–°èŠ‚
                current_section = line[1:-1]
                section_data = {}
            
            # æ£€æŸ¥æ˜¯å¦æ˜¯é”®å€¼å¯¹
            elif '=' in line and current_section:
                key, value = line.split('=', 1)
                section_data[key.strip()] = value.strip()
        
        # å¤„ç†æœ€åä¸€ä¸ªèŠ‚
        if current_section:
            self.sections_processed += 1
            yield {
                'section': current_section,
                'type': 'ini',
                'data': section_data,
                'keys': list(section_data.keys())
            }
    
    def process_config(self, content: str) -> Generator[Dict[str, Any], None, None]:
        """å¤„ç†é…ç½®æ–‡ä»¶å†…å®¹"""
        if self.config_format == 'json':
            yield from self.parse_json_config(content)
        elif self.config_format == 'ini':
            yield from self.parse_ini_config(content)
        else:
            print(f"âŒ ä¸æ”¯æŒçš„é…ç½®æ ¼å¼: {self.config_format}")


def create_sample_log_file():
    """åˆ›å»ºç¤ºä¾‹æ—¥å¿—æ–‡ä»¶"""
    log_entries = [
        "2024-01-20 10:30:45 [INFO] auth: ç”¨æˆ·ç™»å½•æˆåŠŸ (user:user123, ip:192.168.1.100)",
        "2024-01-20 10:31:02 [DEBUG] database: æ‰§è¡ŒæŸ¥è¯¢: SELECT * FROM users",
        "2024-01-20 10:31:15 [WARN] cache: ç¼“å­˜å‘½ä¸­ç‡ä½äºé˜ˆå€¼",
        "2024-01-20 10:31:30 [ERROR] payment: æ”¯ä»˜å¤„ç†å¤±è´¥ (user:user456, ip:192.168.1.101)",
        "2024-01-20 10:31:45 [INFO] api: APIè°ƒç”¨æˆåŠŸ /api/v1/users",
        "# è¿™æ˜¯æ³¨é‡Šè¡Œ",
        "2024-01-20 10:32:00 [INFO] auth: ç”¨æˆ·é€€å‡º (user:user123, ip:192.168.1.100)",
        "æ— æ•ˆçš„æ—¥å¿—è¡Œ",
        "2024-01-20 10:32:15 [CRITICAL] system: ç³»ç»Ÿå†…å­˜ä¸è¶³",
    ]
    
    with open('23. Iterator/sample.log', 'w', encoding='utf-8') as f:
        f.write('\n'.join(log_entries))
    print("ğŸ“ å·²åˆ›å»ºç¤ºä¾‹æ—¥å¿—æ–‡ä»¶: sample.log")


def demo_web_crawler():
    """æ¼”ç¤ºç½‘ç»œçˆ¬è™«è¿­ä»£å™¨"""
    print("=" * 50)
    print("ğŸ•·ï¸ ç½‘ç»œçˆ¬è™«è¿­ä»£å™¨æ¼”ç¤º")
    print("=" * 50)
    
    # åˆ›å»ºçˆ¬è™«è¿­ä»£å™¨
    crawler = WebCrawlerIterator("https://api.example.com/articles", total_pages=3, page_size=5)
    
    print("å¼€å§‹çˆ¬å–æ•°æ®...")
    articles = []
    
    for article in crawler:
        articles.append(article)
        print(f"  ğŸ“„ {article['title']} (ä½œè€…: {article['author']}, æµè§ˆ: {article['view_count']})")
        
        # åªæ˜¾ç¤ºå‰å‡ æ¡
        if len(articles) >= 8:
            print("  ... (çœç•¥æ›´å¤šæ•°æ®)")
            break
    
    print(f"\nğŸ“Š çˆ¬å–ç»Ÿè®¡: å…±è·å– {len(articles)} ç¯‡æ–‡ç« ")


def demo_log_analysis():
    """æ¼”ç¤ºæ—¥å¿—æ–‡ä»¶åˆ†æ"""
    print("\n" + "=" * 50)
    print("ğŸ“‹ æ—¥å¿—æ–‡ä»¶åˆ†ææ¼”ç¤º")
    print("=" * 50)
    
    # åˆ›å»ºç¤ºä¾‹æ—¥å¿—æ–‡ä»¶
    create_sample_log_file()
    
    # åˆ›å»ºæ—¥å¿—è¿­ä»£å™¨
    log_parser = LogFileIterator()
    
    print("å¼€å§‹åˆ†ææ—¥å¿—æ–‡ä»¶...")
    log_stats = {'INFO': 0, 'DEBUG': 0, 'WARN': 0, 'ERROR': 0, 'CRITICAL': 0}
    user_activities = {}
    
    for log_entry in log_parser.process_file('23. Iterator/sample.log'):
        # ç»Ÿè®¡æ—¥å¿—çº§åˆ«
        log_stats[log_entry.level] = log_stats.get(log_entry.level, 0) + 1
        
        # ç»Ÿè®¡ç”¨æˆ·æ´»åŠ¨
        if log_entry.user_id:
            if log_entry.user_id not in user_activities:
                user_activities[log_entry.user_id] = []
            user_activities[log_entry.user_id].append(log_entry.message)
        
        print(f"  ğŸ“ [{log_entry.level}] {log_entry.timestamp.strftime('%H:%M:%S')} - {log_entry.message}")
    
    print(f"\nğŸ“Š æ—¥å¿—ç»Ÿè®¡:")
    for level, count in log_stats.items():
        if count > 0:
            print(f"  {level}: {count} æ¡")
    
    print(f"\nğŸ‘¥ ç”¨æˆ·æ´»åŠ¨:")
    for user_id, activities in user_activities.items():
        print(f"  {user_id}: {len(activities)} æ¬¡æ´»åŠ¨")


def demo_database_processing():
    """æ¼”ç¤ºæ•°æ®åº“æ‰¹é‡å¤„ç†"""
    print("\n" + "=" * 50)
    print("ğŸ—„ï¸ æ•°æ®åº“æ‰¹é‡å¤„ç†æ¼”ç¤º")
    print("=" * 50)
    
    # åˆ›å»ºæ•°æ®åº“è¿­ä»£å™¨
    db_iter = DatabaseBatchIterator("users", batch_size=500)
    
    print("å¼€å§‹æ‰¹é‡å¤„ç†æ•°æ®åº“è®°å½•...")
    
    department_stats = {}
    salary_sum = 0
    record_count = 0
    
    try:
        for batch in db_iter:
            # å¤„ç†å½“å‰æ‰¹æ¬¡
            for record in batch:
                # ç»Ÿè®¡éƒ¨é—¨
                dept = record['department']
                department_stats[dept] = department_stats.get(dept, 0) + 1
                
                # ç´¯è®¡è–ªèµ„
                salary_sum += record['salary']
                record_count += 1
            
            print(f"  ğŸ“¦ å¤„ç†æ‰¹æ¬¡: {len(batch)} æ¡è®°å½•")
            
            # åªå¤„ç†å‰å‡ æ‰¹ï¼Œé¿å…è¾“å‡ºè¿‡å¤š
            if record_count >= 2000:
                print("  ... (çœç•¥æ›´å¤šæ‰¹æ¬¡)")
                break
    
    finally:
        db_iter.close()
    
    print(f"\nğŸ“Š å¤„ç†ç»Ÿè®¡:")
    print(f"  æ€»è®°å½•æ•°: {record_count}")
    print(f"  å¹³å‡è–ªèµ„: {salary_sum / record_count:.2f}" if record_count > 0 else "  å¹³å‡è–ªèµ„: 0")
    print(f"  éƒ¨é—¨åˆ†å¸ƒ: {dict(list(department_stats.items())[:5])}")


def demo_config_parsing():
    """æ¼”ç¤ºé…ç½®æ–‡ä»¶è§£æ"""
    print("\n" + "=" * 50)
    print("âš™ï¸ é…ç½®æ–‡ä»¶è§£ææ¼”ç¤º")
    print("=" * 50)
    
    # JSONé…ç½®ç¤ºä¾‹
    json_config = """
    {
        "database": {
            "host": "localhost",
            "port": 5432,
            "name": "myapp",
            "user": "admin"
        },
        "cache": {
            "type": "redis",
            "host": "localhost",
            "port": 6379,
            "ttl": 3600
        },
        "logging": {
            "level": "INFO",
            "file": "/var/log/app.log",
            "max_size": "100MB"
        }
    }
    """
    
    print("è§£æJSONé…ç½®:")
    json_parser = ConfigFileIterator('json')
    for section in json_parser.process_config(json_config):
        print(f"  ğŸ“ [{section['section']}] - {len(section['keys'])} ä¸ªé…ç½®é¡¹")
        for key in section['keys']:
            print(f"    {key}: {section['data'][key]}")
    
    # INIé…ç½®ç¤ºä¾‹
    ini_config = """
    [database]
    host = localhost
    port = 5432
    name = myapp
    user = admin
    
    [cache]
    type = redis
    host = localhost
    port = 6379
    ttl = 3600
    
    # æ—¥å¿—é…ç½®
    [logging]
    level = INFO
    file = /var/log/app.log
    max_size = 100MB
    """
    
    print(f"\nè§£æINIé…ç½®:")
    ini_parser = ConfigFileIterator('ini')
    for section in ini_parser.process_config(ini_config):
        print(f"  ğŸ“ [{section['section']}] - {len(section['keys'])} ä¸ªé…ç½®é¡¹")
        for key in section['keys']:
            print(f"    {key}: {section['data'][key]}")


if __name__ == "__main__":
    print("ğŸ¯ è¿­ä»£å™¨æ¨¡å¼å®é™…åº”ç”¨æ¼”ç¤º")
    
    # è¿è¡Œæ‰€æœ‰æ¼”ç¤º
    demo_web_crawler()
    demo_log_analysis()
    demo_database_processing()
    demo_config_parsing()
    
    print("\n" + "=" * 50)
    print("âœ… æ¼”ç¤ºå®Œæˆï¼")
    print("ğŸ’¡ æç¤º: è¿™äº›ç¤ºä¾‹å±•ç¤ºäº†è¿­ä»£å™¨åœ¨å®é™…é¡¹ç›®ä¸­çš„å¼ºå¤§åº”ç”¨")
    print("=" * 50)
